\section{Teorier}

\subsection{Teoretisk beskrivelse}
En av de sentrale teoriene innen informasjonsgjenfinning er 'Vector Space Model'. Dette er en algebraisk modell, som representerer dokumenter som vektorer av sine indekserte termer. Prosedyren som denne modellen tar i bruk, er delt i tre deler:
\begin{enumerate}
\item{Dokumentindeksering er å trekke ut termer basert på innholdet.}
\item{Vekting av de indekserte termene, ved hjelp av forekomstfrekvens for termene}
\item{Rangering av dokumentene med hensyn til spørringen, vektet av en likhetsmåling}
\end{enumerate}
I denne modellen representeres både dokumenter og spørringer ved en vektor, på formen: \[d_j = (w_{1,j}, w_{2,j}, ..., w_{n,j})
\]
\[ 
q = (w_{1,q}, w_{2,q}, ..., w_{t,q})
\]
hvor hver enkelt dimensjon representerer en separat term. Begrepet 'term' kan variere noe. En term kan representere en setning, et ord eller en del av en setning. I vår oppgave har vi valgt å håndtere dem som ord, for å gjøre det enklere for oss å indeksere. Ved indeksering velger man alltid å representere kun de termene som er representert i dokumentene. Disse termene har derfor en verdi over null. Verdien er som regel vektet ved hjelp av en funksjon som kalkulerer verdien basert på frekvensen av termen. Den mest brukte funksjonen er tf*idf, 'term-frequency* inverse- document- frequency'. Denne måten å vekte termer på forteller oss hvor viktig et ord er i forhold til dokumentet det er en del av, og i forhold til mengden av dokumenter i samlingen det søkes i. Tf*idf-verdien vil øke proporsjonalt med antallet ganger ordet forekommer i dokumentet, men justeres ved hjelp av antallet forekomster i resten av dokumentene. Dette blir gjort for å ta hensyn til ofte brukte ord, som kan forekomme ofte i alle dokumentene kontra ord som forekommer kun i et spesifikt dokument. 

\paragraph{}
Når dokumentene indekseres, forsøker vi å fjerne ordene som ikke beskriver innholdet. Eksempler er: dette, er, en, et, og, o.l. Disse ordene kalles "stoppord". 

\paragraph{}
Etter å ha vektet dokumentene, vil metoden rangere resultatene ved hjelp av å regne ut "cosinus-likheten" mellom dokumentene og spørringen. Ved å beregne "cosinus-likheten" menes å sammenligne vektoren for spørringen med de ulike vektorene for dokumentene. Dette gjøres ved å kalkulere vinkelen mellom vektorene. Formelen for dette er gitt ved:
\[
\cos(\theta) = \frac{d_2 \times q}{\parallel d_2 \parallel \times \parallel q \parallel}
\]
hvor telleren betegner vektorproduktet og nevneren betegner normaliseringen. Fordi alle vektorverdiene er positive og aldri 0, vil et resultat på 0 bety ortogonalitet mellom vektorene og det er intet samsvar mellom dem.

Denne modellen har imidlertid enkelte begrensninger:
\begin{itemize}
\item{Store dokumenter representeres på en ubalansert måte, med et lite skalarprodukt og høy dimensjon}
\item{Ikke alle relevant ord vil bli matchet. Substringer kan gi 'falske positiver'}
\item{Synonymer kan forstyrre den semantiske tolkningen av dokumentet}
\item{Når dokumenter indekseres blir rekkefølgen av termer tapt.}
\item{Det antas at det er en statistisk uavhengighet mellom ord}
\end{itemize}


\subsection{Teoriene i praksis}
For å benytte oss av 'VSM' var vi nødt til å gjøre en del forberedende arbeid på dokumentene våre. Pasientjournalene ble gitt som en tekstfil. Det første som måtte gjøres var å fjerne stoppord, for å forbedre indekseringen. Etter fjerningen av stoppord benyttet vi oss av en 'Stemmer' for å stemme alle ordene til sin grunnform. For å utføre stemmingen benyttet vi 'Snowball Stemmer', et program gjort tilgjengelig publisering av kildekoden på nett. 
Etter å ha blitt stemt var dokumentene klare for å bli søkt i.
Teorikapitlene i NLH trengte også preprosessering før det var klart til bruk. Først måtte vi fjerne HTML-merker, for å lettere søke i teksten, for så å gjøre kapitlene om til XML-filer. Teorikapitlene ble også stemt.
ICD10-kodene ble publisert som en .owl-fil. For å lettere filtrere, lagde vi klasser for hver enkelt etikett i ICD10 og ATC.
Etter å ha laget klasser, benyttet vi oss av Apache sitt 'Digester'-bibliotek for hver enkelt etikett. 
Til slutt benyttet vi Apache sitt 'Lucene'-bibliotek for å indeksere alle klassene. De indekserte filene ble lagret på disk, for å slippe å gjøre arbeid om igjen. På denne måten var det mulig for oss å benytte teorikapitlene som søkeparametere når vi søkte gjennom de indekserte filene. 
Som kjent, var målet å finne den beste matchen mellom de pasientjournalene og terapikapitlene i NLH. 
Som tidligere nevnt, benyttet vi VSM-modellen for å utføre søket og rangeringen av dokumentene. Vi fulgte stegene presentert i den teoretiske beskrivelsen til punkt og prikke. Måten vi gjorde det på var som følger:
\begin{itemize}
\item{Bygget et array av preprossesserte teorikapitler}
\item{Vektet ordene i dokumentet, ved hjelp av tf*idf-formelen}
\item{Kalkulerte cosinus-likheten, ved hjelp av ovennevnte formel}
\item{Rangerte dokumentene}
\end{itemize}

\subsection{Kommentarer til løsningen}
Vi valgte å dele NLH i kapitler og subkapitler, da dette var måten HTML-filene var delt inn på. Det eksisterer subsubkapitler innenfor subkapitlene, men subsubkapitlene er ikke separate filer. Vi fant ingen god måte å løse dette problemet på. 