\section{Architecture}
\label{sec:architecture}

Etter undersøkelse bestemte vi oss for å bygge systemet vårt ved hjelp av en 'rør og filter'-arkitektur (pipe and filter). Det gjorde det lettere for oss å kunne jobbe på filer individuelt og unngå dobbeltarbeid, da vi kunne lagre resultatene fra en operasjon før vi utførte neste operasjon. Siden pasientjournalene og den norske legemiddelhåndboken kun skulle tolkes en gang, mens søkene skulle utføres gjentatte ganger, med ulike søkeparametere, var dette den mest hensiktsmessige løsningen.

%TODO: Bilde av arkitekturen

En viktig bit av søk i dokumentsamlinger er å kunne indeksere termer i samlingen av dokumenter. For å gjøre dette, tolket vi ATC- og ICD10-kodene og lagret resultatene. Ved å preprosessere NLH og pasientjournalene, var det mulig for søkemotoren å bygge en spørring basert på termer i pasientjournalen. Denne spørringen ble dermed brukt for å søke i ATC- og ICD10-indeksene. Ved å bruke java-biblioteket Apache Lucene, implementerte vi en metode for å rangere resultatene basert på sammenhengen mot ATC-/ICD10-kodene.

Merk at javakoden er skrevet på engelsk. Vi valgte engelsk fordi det er en uskrevet standard at man programmerer på engelsk. Det gjorde det også vesentlig enklere å kunne spørre om hjelp via nett.

\begin{description}
\item{\textbf{ATC/ATCParser:}}\\
ATC-kodene ble gitt av fagstaben som en .owl-fil. ATC benyttes for å lese en prologfil. Ved å lese filen, kunne 'ATCParseren' separere innholdet og lagre det som separate ATC-objektene for enklere søk.
\item{\textbf{ICD/ICDParser:}}\\
Legemiddelhåndoken er åpent tilgjengelig på nett, som HTML-filer. 'NLHParseren' leste filene sekvensielt og fjernet HTML-etiketter fra filene, for å fjerne støy. Etter å ha fjernet HTML-etiketter ble hvert dokument gjort om til en XML-fil. XML-filene ble deretter lest sekvensielt for å fjerne stoppord og deretter stemt. For å fjerne stoppord og stemme teksten benyttet vi et åpent tilgjengelig stemmeprogram ved navn 'SnowballStemmer'. 
%TODO: Add reference)
\item{\textbf{Pasientjournaler:}}\\
Pasientjournalene ble gitt som en docx-fil, av fagstabel. Vi delte filen i separate filer for hver pasientjournal. Ved hjelp av javaklassen 'CaseReader', som vi implementerte på egenhånd, fjernet vi stoppord og stemte pasientjournalene.
\item{\texfbf{Indeksering:}}\\
Ved hjelp av java-biblioteket Apache Lucene %TODO: Add reference 
leste vi inn ATC-kodene og indekserte hver etikett og hver kode. Programmet indekserte også hver ICD10-etikett, -kode og synonymer for kodene. Dette ble lagret i separate filer, slik at det ble lettere å aksessere i ettertid.
\item{\textbf{Søk:}}\\
Med pasientjournalene og NLH omgjort til en felles standard, var det vesentlig enklere å gjøre søkearbeid. Søkemotoren tar en setning fra hver pasientjournal og kapittel fra NLH og søker gjennom ATC- og ICD10-kodene for å finne relevante treff. Dette ble gjort ved hjelp av Apache Lucene, for å enklere lese indekseringen og senere rangere treffene.
\item{\textbf{vectorSpace package:}}\\
Vi valgte å benytte 'Vector Space Model' for søk og rangering. Implementasjonen av dette er nærmere beskrevet i Teorier %Add link
\begin{description}
\item{textit{DocumentVector: }} \\
DocumentVector-klassen er brukt for å skape vektorer av de indekserte dokumentene. Klassen benytter en 'DocumentCollection'-lytter for å unngå av det oppstår duplikate vektorer.
\item{textit{DocumentCollection: }}\\
'DocumentCollection'-klassen lager en samling av vektorene produsert av 'DocumentVector'. Dette lagres som et 'HashMap'. Klassen og samlingen oppdateres hver gang et nytt dokument er lagt til i samlingen. 
\item{textit{VectorSpace: }}\\
Denne klassen utfører en beregning for å kalkulere cosinus-likheten mellom vektorer. Mer om hvordan dette beregnes er beskrevet i teorier %Add link
\item{textit{VectorMain: }}\\
Denne klassen består av 'main'-metoden for å kjøre programmet. Denne klassen benytter seg av alle de tidligere beskrevne klassene for å utføre en spørring som en helhet.
\end{description}
\end{description}